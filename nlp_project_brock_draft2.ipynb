{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard DS imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Vizualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP imports\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Custom imports\n",
    "import acquire as a\n",
    "import env\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = a.process_all_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Steps:\n",
    "- Lowercase All Text\n",
    "- Remove accented characters and non-ASCII characters\n",
    "- Remove special characters\n",
    "- Tokenize\n",
    "- Lemmatize\n",
    "- Remove Stopwords\n",
    "- Add extra Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def clean(text):\\n    ‘A simple function to cleanup text data’\\n    wnl = nltk.stem.WordNetLemmatizer()\\n    stopwords = nltk.corpus.stopwords.words(‘english’)\\n    text = (unicodedata.normalize(‘NFKD’, text)\\n             .encode(‘ascii’, ‘ignore’)\\n             .decode(‘utf-8’, ‘ignore’)\\n             .lower())\\n    text = text.replace(‘/’, ' ‘)\\n    text = text.replace(‘-’, ' ‘)\\n    words = re.sub(r”[^a-z0-9’+\\\\s]“, ‘’, text).split()\\n    return [wnl.lemmatize(word) for word in words if word not in stopwords]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def clean(text):\n",
    "    ‘A simple function to cleanup text data’\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words(‘english’)\n",
    "    text = (unicodedata.normalize(‘NFKD’, text)\n",
    "             .encode(‘ascii’, ‘ignore’)\n",
    "             .decode(‘utf-8’, ‘ignore’)\n",
    "             .lower())\n",
    "    text = text.replace(‘/’, ' ‘)\n",
    "    text = text.replace(‘-’, ' ‘)\n",
    "    words = re.sub(r”[^a-z0-9’+\\s]“, ‘’, text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in the original text.\n",
    "    The text is all lowercased, \n",
    "    the text is encoded in ascii and any characters that are not ascii are ignored.\n",
    "    The text is then decoded in utf-8 and any characters that are not ascii are ignored\n",
    "    Additionally, special characters are all removed.\n",
    "    A clean article is then returned\n",
    "    '''\n",
    "    #lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    #normalize\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    string = string.replace('/',' ')\n",
    "    string = string.replace('-',' ')\n",
    "    #remove special characters and replaces it with blank\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string\n",
    "    and returns the string as individual tokens put back into the string\n",
    "    '''\n",
    "    #create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    #use the tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in a string\n",
    "    and returns the lemmatized word joined back into the string\n",
    "    '''\n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    #look at the article \n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    #join lemmatized words into article\n",
    "    string = ' '.join(lemmas)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in text, extra words and exclude words\n",
    "    and returns a list of text with stopword removed\n",
    "    '''\n",
    "    #create stopword list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    #remove excluded words from list\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    #add the extra words to the list\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    #split the string into different words\n",
    "    words = string.split()\n",
    "    \n",
    "    #create a list of words that are not in the list\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    #join the words that are not stopwords (filtered words) back into the string\n",
    "    string = ' '.join(filtered_words)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    df = df.rename(columns={'readme_contents':'original'})\n",
    "    # df['clean'] = cleaned and tokenized version with stopwords removed\n",
    "    df['clean'] = df['original'].apply(basic_clean\n",
    "                                      ).apply(tokenize\n",
    "                                             ).apply(remove_stopwords)\n",
    "    # df['lemmatized'] = lemmatized version of clean data\n",
    "    df['lematized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    for i, x in enumerate(df.lematized):\n",
    "        df['lematized'][i] = x.split()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    df = df.rename(columns={'readme_contents':'original'})\n",
    "    # df['clean'] = cleaned and tokenized version with stopwords removed\n",
    "    df['clean'] = df['original'].apply(basic_clean\n",
    "                                      ).apply(tokenize\n",
    "                                             ).apply(remove_stopwords)\n",
    "    # df['lematized'] = lemmatized version of clean data\n",
    "    df['lematized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    # Split lemmatized strings into lists of words\n",
    "    df['lematized'] = df['lematized'].apply(lambda x: x.split())\n",
    "    \n",
    "    # Drop strings longer than 15 characters\n",
    "    df['lematized'] = df['lematized'].apply(lambda x: [word for word in x if len(word) <= 15])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, extra_stopwords= ['repository', 'githubcom', 'host']):\n",
    "    df = df.rename(columns={'readme_contents':'original'})\n",
    "    # df['clean'] = cleaned and tokenized version with stopwords removed\n",
    "    df['clean'] = df['original'].apply(basic_clean\n",
    "                                      ).apply(tokenize\n",
    "                                             ).apply(remove_stopwords, extra_words=extra_stopwords)\n",
    "    # df['lematized'] = lemmatized version of clean data\n",
    "    df['lematized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, extra_stopwords= ['repository', 'githubcom', 'host']):\n",
    "    df = df.rename(columns={'readme_contents':'original'})\n",
    "    # df['clean'] = cleaned and tokenized version with stopwords removed\n",
    "    df['clean'] = df['original'].apply(basic_clean\n",
    "                                      ).apply(tokenize\n",
    "                                             ).apply(remove_stopwords, extra_words=extra_stopwords)\n",
    "    # df['lematized'] = lemmatized version of clean data\n",
    "    df['lematized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    # Split lemmatized strings into lists of words\n",
    "    df['lematized'] = df['lematized'].apply(lambda x: x.split())\n",
    "    \n",
    "    # Drop words longer than 15 characters\n",
    "    df['lematized'] = df['lematized'].apply(lambda x: [word for word in x if len(word) <= 15])\n",
    "    \n",
    "    # Join lists of words back into strings\n",
    "    df['lematized'] = df['lematized'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/brockgreen/codeup-data-science/1800-readme/nlp_project_brock_draft2.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brockgreen/codeup-data-science/1800-readme/nlp_project_brock_draft2.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mlematized[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "df.lematized[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(df.lematized[10]):\n",
    "    for content in x:\n",
    "        if len(content) > 15:\n",
    "            x.replace(content, np.nan)\n",
    "            df.lematized[10][i].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.lematized[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repository',\n",
       " 'host',\n",
       " 'code',\n",
       " 'datasets',\n",
       " 'relating',\n",
       " 'responsible',\n",
       " 'nlp',\n",
       " 'project',\n",
       " 'meta',\n",
       " 'ai',\n",
       " 'project',\n",
       " 'holisticbiashttpsgithubcomfacebookresearchresponsiblenlptreemainholisticbias',\n",
       " 'eric',\n",
       " 'michael',\n",
       " 'smith',\n",
       " 'melissa',\n",
       " 'hall',\n",
       " 'melanie',\n",
       " 'kambadur',\n",
       " 'eleonora',\n",
       " 'presani',\n",
       " 'adina',\n",
       " 'williams',\n",
       " \"'\",\n",
       " 'sorry',\n",
       " 'hear',\n",
       " 'finding',\n",
       " 'bias',\n",
       " 'language',\n",
       " 'model',\n",
       " 'holistic',\n",
       " 'descriptor',\n",
       " 'dataset',\n",
       " '2022httpsarxivorgpdf220509209pdf',\n",
       " 'code',\n",
       " 'generate',\n",
       " 'dataset',\n",
       " 'holisticbias',\n",
       " 'consisting',\n",
       " 'nearly',\n",
       " '600',\n",
       " 'demographic',\n",
       " 'term',\n",
       " '450k',\n",
       " 'sentence',\n",
       " 'prompt',\n",
       " 'code',\n",
       " 'calculate',\n",
       " 'likelihood',\n",
       " 'bias',\n",
       " 'metric',\n",
       " 'amount',\n",
       " 'bias',\n",
       " 'language',\n",
       " 'model',\n",
       " 'defined',\n",
       " 'holisticbias',\n",
       " 'demographic',\n",
       " 'term',\n",
       " 'fairscorehttpsgithubcomfacebookresearchresponsiblenlptreemainfairscore',\n",
       " 'rebecca',\n",
       " 'qian',\n",
       " 'candace',\n",
       " 'ross',\n",
       " 'jude',\n",
       " 'fernandes',\n",
       " 'eric',\n",
       " 'smith',\n",
       " 'douwe',\n",
       " 'kiela',\n",
       " 'adina',\n",
       " 'williams',\n",
       " 'perturbation',\n",
       " 'augmentation',\n",
       " 'fairer',\n",
       " 'nlp',\n",
       " '2022httpsaclanthologyorg2022emnlpmain646',\n",
       " 'panda',\n",
       " 'annotated',\n",
       " 'dataset',\n",
       " '100k',\n",
       " 'demographic',\n",
       " 'perturbation',\n",
       " 'diverse',\n",
       " 'text',\n",
       " 'rewritten',\n",
       " 'change',\n",
       " 'gender',\n",
       " 'raceethnicity',\n",
       " 'age',\n",
       " 'reference',\n",
       " 'perturber',\n",
       " 'pretrained',\n",
       " 'model',\n",
       " 'code',\n",
       " 'artifact',\n",
       " 'related',\n",
       " 'perturbation',\n",
       " 'augmentation',\n",
       " 'fairer',\n",
       " 'nlp',\n",
       " 'project',\n",
       " 'released',\n",
       " 'shortly',\n",
       " 'advpromptsethttpsgithubcomfacebookresearchresponsiblenlptreemainadvpromptset',\n",
       " 'advpromptset',\n",
       " 'comprehensive',\n",
       " 'challenging',\n",
       " 'adversarial',\n",
       " 'text',\n",
       " 'prompt',\n",
       " 'set',\n",
       " '197628',\n",
       " 'prompt',\n",
       " 'varying',\n",
       " 'toxicity',\n",
       " 'level',\n",
       " '24',\n",
       " 'sensitive',\n",
       " 'demographic',\n",
       " 'identity',\n",
       " 'group',\n",
       " 'combination',\n",
       " 'gendergap',\n",
       " 'pipelinehttpsgithubcomfacebookresearchresponsiblenlptreemaingendergappipeline',\n",
       " 'benjamin',\n",
       " 'muller',\n",
       " 'belen',\n",
       " 'alastruey',\n",
       " 'prangthip',\n",
       " 'hansanti',\n",
       " 'elahe',\n",
       " 'kalbassi',\n",
       " 'christophe',\n",
       " 'roper',\n",
       " 'eric',\n",
       " 'michael',\n",
       " 'smith',\n",
       " 'adina',\n",
       " 'williams',\n",
       " 'luke',\n",
       " 'zettlemoyer',\n",
       " 'pierre',\n",
       " 'andrew',\n",
       " 'marta',\n",
       " 'r',\n",
       " 'costajussahttpsarxivorgpdf230816871pdf',\n",
       " 'see',\n",
       " 'contributingmdhttpsgithubcomfacebookresearchresponsiblenlpblobmaincontributingmd',\n",
       " 'help',\n",
       " 'see',\n",
       " 'licensehttpsgithubcomfacebookresearchresponsiblenlpblobmainlicense',\n",
       " 'license',\n",
       " 'information']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lematized[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
